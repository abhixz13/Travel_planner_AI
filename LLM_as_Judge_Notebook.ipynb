{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "41653440",
      "metadata": {
        "id": "41653440"
      },
      "source": [
        "# ğŸ§‘â€âš–ï¸ Build a LLM Judge for Translation Evaluation\n",
        "\n",
        "Evaluating the quality of machine-translated text is a critical challenge in natural language processing (NLP). Traditional methods have significant limitations:\n",
        "\n",
        "- **Human evaluation** is accurate but slow, expensive, and often inconsistent.\n",
        "- **Automatic metrics** like BLEU or ROUGE rely on n-gram overlap and fail to capture **meaning, fluency, or hallucinations**.\n",
        "\n",
        "In this notebook, we build an **LLM-based evaluator** â€” a \"judge\" â€” that uses a Large Language Model (like GPT-4) to assess translations in a **human-like, structured way**.\n",
        "\n",
        "The judge evaluates each translation across three key dimensions:\n",
        "\n",
        "- âœ… **Accuracy**: Does it preserve the original meaning?\n",
        "- ğŸ’¬ **Understandability**: Is it fluent and natural in the target language?\n",
        "- ğŸš« **Hallucination**: Are there added, omitted, or distorted details?\n",
        "\n",
        "Weâ€™ll also explore **back-translation** as an optional validation method to detect semantic inconsistencies.\n",
        "\n",
        "By the end, youâ€™ll have a reusable framework for **automated, reliable, and interpretable translation evaluation** using LLMs."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "72fbb752",
      "metadata": {
        "id": "72fbb752"
      },
      "source": [
        "## ğŸ› ï¸ Step 1: Install and Import Dependencies\n",
        "We begin by installing and importing the required Python libraries.\n",
        "\n",
        "- **openai:** To interact with OpenAI's LLMs via API.\n",
        "\n",
        "- **tiktoken:** For token counting (optional, useful for cost estimation).\n",
        "\n",
        "- **tqdm:** To display progress bars during batch processing.\n",
        "\n",
        "This ensures everything is ready before we implement the judge.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "163f9524",
      "metadata": {
        "id": "163f9524",
        "outputId": "cb17c3d8-cec7-4d95-cfb9-33cd970083b9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.12/dist-packages (1.109.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai) (4.11.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.11.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from openai) (2.11.10)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.12/dist-packages (from openai) (4.15.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install --quiet openai tiktoken tqdm\n",
        "\n",
        "# Import essential libraries\n",
        "%pip install openai tqdm\n",
        "from openai import OpenAI # Official OpenAI client for API calls\n",
        "\n",
        "#import openai\n",
        "import os\n",
        "from tqdm import tqdm # Progress bar for loops"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bcc86336",
      "metadata": {
        "id": "bcc86336"
      },
      "source": [
        "## ğŸ” Step 2: Set Your OpenAI API Key\n",
        "To access the OpenAI models, we need to authenticate using an API key.\n",
        "\n",
        "- We use Colabâ€™s userdata or getpass so that the key is not hard-coded.\n",
        "\n",
        "- The key is stored as an environment variable (OPENAI_API_KEY).\n",
        "\n",
        "This ensures the key remains secure while allowing the OpenAI client to authenticate automatically."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "566c5363",
      "metadata": {
        "id": "566c5363",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ab060bd-aa06-4bbf-a49c-e5594ecf1473"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your OpenAI API key: Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n"
          ]
        }
      ],
      "source": [
        "from getpass import getpass # Secure input for API key\n",
        "from google.colab import userdata  # Colab-specific secure key storage\n",
        "import os\n",
        "from openai import OpenAI\n",
        "\n",
        "#openai.api_key = \"sk-proj------\"  # TODO: insert your OpenAI key securely\n",
        "#os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = getpass('Enter your OpenAI API key: ')\n",
        "client = OpenAI(api_key=os.environ['OPENAI_API_KEY'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da24ebc5",
      "metadata": {
        "id": "da24ebc5"
      },
      "source": [
        "## ğŸ“¥ Step 3: Prepare a Sample Dataset\n",
        "Before testing our LLM judge, we need some translation data.  \n",
        "In this step, we prepare a **small sample dataset** containing:\n",
        "\n",
        "- **Source sentences** (original text).  \n",
        "- **Target translations** (machine-generated translations).  \n",
        "\n",
        "This dataset will be used to run evaluations and check how well the translations perform according to the LLM judge."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fdf3c783",
      "metadata": {
        "id": "fdf3c783"
      },
      "outputs": [],
      "source": [
        "sample_data = [\n",
        "    {\n",
        "        \"source_text\": \"We need to finalize the contract before the end of the month.\",\n",
        "        \"translated_text\": \"æœˆæœ«ã¾ã§ã«å¥‘ç´„ã‚’ç¢ºå®šã•ã›ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚\"\n",
        "    },\n",
        "    {\n",
        "        \"source_text\": \"Schedule a meeting with the design team for next Tuesday.\",\n",
        "        \"translated_text\": \"æ¥é€±ã®æ—¥æ›œæ—¥ã«ãƒ‡ã‚¶ã‚¤ãƒ³ãƒãƒ¼ãƒ ã¨ã®ä¼šè­°ã‚’äºˆå®šã—ã¦ãã ã•ã„ã€‚\"\n",
        "    },\n",
        "    {\n",
        "        \"source_text\": \"All passwords must be changed every 90 days.\",\n",
        "        \"translated_text\": \"ã™ã¹ã¦ã®ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ã¯91æ—¥ã”ã¨ã«å¤‰æ›´ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚\"\n",
        "    }\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##ğŸ“Œ Purpose of This Dataset:\n",
        "\n",
        "- First example: Correct translation (baseline).\n",
        "- Second example: **Critical error** â€” \"Tuesday\" â†’ \"Sunday\".\n",
        "- Third example: **Minor numerical error** â€” \"90 days\" â†’ \"91 days\".\n",
        "These allow us to test the judgeâ€™s sensitivity to different types of translation flaws."
      ],
      "metadata": {
        "id": "f2U_nQXXFZP-"
      },
      "id": "f2U_nQXXFZP-"
    },
    {
      "cell_type": "markdown",
      "id": "c86ef13a",
      "metadata": {
        "id": "c86ef13a"
      },
      "source": [
        "## ğŸ¤– Step 4: Define the LLM Judge Prompt with Few-Shot Examples\n",
        "Now that we have a dataset, we need to define the **evaluation prompt** that guides the LLM to produce consistent, interpretable judgments.\n",
        "\n",
        "Instead of giving the model only instructions, we also include **few-shot examples** (sample inputs and expected judgments).  \n",
        "\n",
        "Why use few-shot examples?  \n",
        "- They **guide the LLM** on how to structure its output.  \n",
        "- They help ensure the judge consistently provides scores for:  \n",
        "  - **Accuracy** (faithfulness to the source)  \n",
        "  - **Understandability** (fluency and clarity)  \n",
        "  - **Hallucination** (added or missing information)  \n",
        "\n",
        "By showing examples of evaluations, the LLM learns the **format and criteria** it should follow for new inputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23233632",
      "metadata": {
        "id": "23233632"
      },
      "outputs": [],
      "source": [
        "\n",
        "def build_judge_prompt(source_text, translated_text):\n",
        "    \"\"\"\n",
        "    Constructs a few-shot evaluation prompt for the LLM judge.\n",
        "\n",
        "    Args:\n",
        "        source_text (str): Original sentence in English.\n",
        "        translated_text (str): Translated sentence in Japanese.\n",
        "\n",
        "    Returns:\n",
        "        str: Complete prompt with examples and current task.\n",
        "    \"\"\"\n",
        "    # Define few-shot examples to guide the LLM\n",
        "    few_shot_examples = \"\"\"Example 1:\n",
        "Original: Please send the invoice by Friday.\n",
        "Translation: é‡‘æ›œæ—¥ã¾ã§ã«è«‹æ±‚æ›¸ã‚’é€ã£ã¦ãã ã•ã„ã€‚\n",
        "\n",
        "Evaluation:\n",
        "{\n",
        "  \"accuracy_score\": 5,\n",
        "  \"accuracy_notes\": \"All details, including deadline and intent, are preserved.\",\n",
        "  \"understandability_score\": 5,\n",
        "  \"understandability_notes\": \"Fluent and idiomatic phrasing.\",\n",
        "  \"hallucination_score\": 5,\n",
        "  \"hallucination_notes\": \"No content was added or omitted.\"\n",
        "}\n",
        "\n",
        "---\n",
        "\n",
        "Example 2:\n",
        "Original: Transfer $2000 to Mike by noon.\n",
        "Translation: ãƒã‚¤ã‚¯ã«2000ãƒ‰ãƒ«ã‚’é€ã£ã¦ãã ã•ã„ã€‚\n",
        "\n",
        "Evaluation:\n",
        "{\n",
        "  \"accuracy_score\": 3,\n",
        "  \"accuracy_notes\": \"Correct recipient and amount, but deadline 'by noon' is missing.\",\n",
        "  \"understandability_score\": 5,\n",
        "  \"understandability_notes\": \"Clear and fluent phrasing.\",\n",
        "  \"hallucination_score\": 4,\n",
        "  \"hallucination_notes\": \"Omission of deadline makes it slightly inaccurate.\"\n",
        "}\n",
        "\n",
        "---\n",
        "\n",
        "Example 3:\n",
        "Original: Do not share this file with anyone outside the company.\n",
        "Translation: ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª°ã¨ã‚‚å…±æœ‰ã—ãªã„ã§ãã ã•ã„ã€‚\n",
        "\n",
        "Evaluation:\n",
        "{\n",
        "  \"accuracy_score\": 4,\n",
        "  \"accuracy_notes\": \"Prohibition is correctly translated, but lacks explicit mention of 'outside the company'.\",\n",
        "  \"understandability_score\": 5,\n",
        "  \"understandability_notes\": \"Fluent and natural Japanese.\",\n",
        "  \"hallucination_score\": 4,\n",
        "  \"hallucination_notes\": \"Missing detail about 'outside the company'.\"\n",
        "}\n",
        "\n",
        "---\"\"\"\n",
        "    # Task instruction for the current translation pair\n",
        "    task = f\"\"\"\n",
        "Now evaluate this case:\n",
        "\n",
        "Original: {source_text}\n",
        "Translation: {translated_text}\n",
        "\n",
        "Respond in the following JSON format:\n",
        "\n",
        "{{\n",
        "  \"accuracy_score\": ...,\n",
        "  \"accuracy_notes\": \"...\",\n",
        "  \"understandability_score\": ...,\n",
        "  \"understandability_notes\": \"...\",\n",
        "  \"hallucination_score\": ...,\n",
        "  \"hallucination_notes\": \"...\"\n",
        "}}\n",
        "\"\"\"\n",
        "    # Combine examples and task\n",
        "    return few_shot_examples + task\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "330bd480",
      "metadata": {
        "id": "330bd480"
      },
      "source": [
        "## ğŸ§ª Step 5: Run the Judge on Sample Data\n",
        "With the dataset prepared and the evaluation prompt defined, we are now ready to **run the LLM judge**.\n",
        "\n",
        "In this step:\n",
        "1. We take sentences from our **sample dataset**.  \n",
        "2. We pass each example (source + translation) through the **LLM judge prompt**.  \n",
        "3. The LLM returns a structured evaluation containing scores for:  \n",
        "   - **Accuracy**  \n",
        "   - **Understandability**  \n",
        "   - **Hallucination**  \n",
        "\n",
        "This step validates whether the LLM is correctly applying the evaluation framework we defined."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f8c650ac",
      "metadata": {
        "id": "f8c650ac"
      },
      "outputs": [],
      "source": [
        "\n",
        "def evaluate_with_llm(sample_data):\n",
        "    \"\"\"\n",
        "    Evaluates each translation in the dataset using the LLM judge.\n",
        "\n",
        "    Args:\n",
        "        sample_data (list): List of dictionaries with 'source_text' and 'translated_text'.\n",
        "    \"\"\"\n",
        "    for item in sample_data:\n",
        "        # Generate the full evaluation prompt\n",
        "        prompt = build_judge_prompt(item[\"source_text\"], item[\"translated_text\"])\n",
        "\n",
        "        # Display the input being evaluated\n",
        "        print(\"ğŸ” Evaluating:\")\n",
        "        print(f\"Original: {item['source_text']}\")\n",
        "        print(f\"Translation: {item['translated_text']}\")\n",
        "        print(\"ğŸ§  GPT Response:\")\n",
        "\n",
        "        # Call the OpenAI API\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"gpt-4\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are a strict bilingual translation evaluator.\"},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ],\n",
        "            temperature=0.2 # Low temperature for deterministic, consistent outputs\n",
        "        )\n",
        "         # Print the LLM's structured evaluation\n",
        "        print(response.choices[0].message.content)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a09f1dfe",
      "metadata": {
        "id": "a09f1dfe"
      },
      "source": [
        "## âœ… Step 6: Try It Out\n",
        "\n",
        "Now that the LLM judge is working on our sample dataset, itâ€™s time to **experiment with it directly**.\n",
        "\n",
        "In this step:\n",
        "- You can provide **your own source sentences and translations** to test.  \n",
        "- The LLM judge will evaluate them using the same framework (Accuracy, Understandability, Hallucination).  \n",
        "- This helps explore how well the judge generalizes to new inputs beyond the prepared dataset.  \n",
        "\n",
        "Think of this step as a **playground** for testing translations and observing how the LLM judge responds.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f90b4a1",
      "metadata": {
        "id": "4f90b4a1",
        "outputId": "dbc308f9-f0c8-411a-ef6f-e4c88a0df68a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ” Evaluating:\n",
            "Original: We need to finalize the contract before the end of the month.\n",
            "Translation: æœˆæœ«ã¾ã§ã«å¥‘ç´„ã‚’ç¢ºå®šã•ã›ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚\n",
            "ğŸ§  GPT Response:\n",
            "{\n",
            "  \"accuracy_score\": 5,\n",
            "  \"accuracy_notes\": \"All details, including the deadline and action, are accurately translated.\",\n",
            "  \"understandability_score\": 5,\n",
            "  \"understandability_notes\": \"The translation is fluent and idiomatic.\",\n",
            "  \"hallucination_score\": 5,\n",
            "  \"hallucination_notes\": \"No content was added or omitted in the translation.\"\n",
            "}\n",
            "ğŸ” Evaluating:\n",
            "Original: Schedule a meeting with the design team for next Tuesday.\n",
            "Translation: æ¥é€±ã®æ—¥æ›œæ—¥ã«ãƒ‡ã‚¶ã‚¤ãƒ³ãƒãƒ¼ãƒ ã¨ã®ä¼šè­°ã‚’äºˆå®šã—ã¦ãã ã•ã„ã€‚\n",
            "ğŸ§  GPT Response:\n",
            "{\n",
            "  \"accuracy_score\": 3,\n",
            "  \"accuracy_notes\": \"The day of the week is incorrect. The original text specifies 'Tuesday', but the translation says 'Sunday'.\",\n",
            "  \"understandability_score\": 5,\n",
            "  \"understandability_notes\": \"Despite the error in day, the sentence is grammatically correct and fluent in Japanese.\",\n",
            "  \"hallucination_score\": 4,\n",
            "  \"hallucination_notes\": \"The translation inaccurately represents 'Tuesday' as 'Sunday'.\"\n",
            "}\n",
            "ğŸ” Evaluating:\n",
            "Original: All passwords must be changed every 90 days.\n",
            "Translation: ã™ã¹ã¦ã®ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ã¯91æ—¥ã”ã¨ã«å¤‰æ›´ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚\n",
            "ğŸ§  GPT Response:\n",
            "{\n",
            "  \"accuracy_score\": 4,\n",
            "  \"accuracy_notes\": \"The translation is mostly accurate, but the time frame is incorrect. It should be '90 days', not '91 days'.\",\n",
            "  \"understandability_score\": 5,\n",
            "  \"understandability_notes\": \"The sentence is grammatically correct and understandable.\",\n",
            "  \"hallucination_score\": 4,\n",
            "  \"hallucination_notes\": \"The translation added an extra day to the original text.\"\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "# Execute evaluation\n",
        "evaluate_with_llm(sample_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ğŸ“ Interpretation\n",
        "\n",
        "The outputs here illustrate how the LLM judge can evaluate translations with nuance, not just surface-level fluency.\n",
        "\n",
        "- **Example 1**: The translation is faithful, fluent, and idiomatic. Both the action (â€œfinalize the contractâ€) and the timeframe (â€œend of the monthâ€) are conveyed exactly.\n",
        "\n",
        "- **Example 2**: The sentence is natural in Japanese, but the day was mistranslated (â€œTuesdayâ€ â†’ â€œSundayâ€). The judge penalizes accuracy while recognizing fluency.\n",
        "\n",
        "- **Example 3**: The only issue is a small numerical slip (90 â†’ 91). The judge downgrades accuracy without flagging hallucination, showing it can separate factual errors from structural ones.\n",
        "\n",
        "ğŸ“Œ **Insight:** The LLM judge distinguishes **semantic fidelity** from **linguistic fluency**. It correctly penalizes meaning-altering errors (wrong day, wrong number) while rewarding clear and natural language â€” something traditional metrics like BLEU often fail to capture.\n"
      ],
      "metadata": {
        "id": "qxq2setaAwjB"
      },
      "id": "qxq2setaAwjB"
    },
    {
      "cell_type": "markdown",
      "id": "18b00004",
      "metadata": {
        "id": "18b00004"
      },
      "source": [
        "## ğŸ” Step 7 (Optional): Add Back-Translation Support\n",
        "Back-translation is an **optional validation technique**:\n",
        "\n",
        "- Translate the target text back into the source language.\n",
        "\n",
        "- Compare it with the original source sentence.\n",
        "\n",
        "Why it helps:\n",
        "\n",
        "- If the back-translation is close to the original, it suggests the translation is faithful.\n",
        "\n",
        "- If not, there may be errors, omissions, or hallucinations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "268d393c",
      "metadata": {
        "id": "268d393c"
      },
      "outputs": [],
      "source": [
        "def get_back_translation(translated_text, target_lang=\"Japanese\"):\n",
        "    \"\"\"\n",
        "    Translates the target-language text back into English.\n",
        "\n",
        "    Args:\n",
        "        translated_text (str): Text in the target language (e.g., Japanese).\n",
        "        target_lang (str): Language of the input text.\n",
        "\n",
        "    Returns:\n",
        "        str: Back-translated English sentence.\n",
        "    \"\"\"\n",
        "    back_prompt = f\"Translate this {target_lang} sentence back into English:\\n\\n{translated_text}\"\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4\",\n",
        "        messages=[{\"role\": \"user\", \"content\": back_prompt}],\n",
        "        temperature=0.3 # Slightly higher for natural rephrasing\n",
        "    )\n",
        "    return response.choices[0].message.content.strip()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e592221",
      "metadata": {
        "id": "9e592221",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70568622-50dc-4e7c-edb6-d37c62685d6d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ” Evaluating:\n",
            "Original: We need to finalize the contract before the end of the month.\n",
            "Translation: æœˆæœ«ã¾ã§ã«å¥‘ç´„ã‚’ç¢ºå®šã•ã›ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚\n",
            "Back-Translation: We need to finalize the contract by the end of the month.\n",
            "ğŸ§  GPT Response:\n",
            "The back-translation perfectly preserves the original meaning. There are no changes in tone, information, or accuracy. The translation is accurate and the message is conveyed correctly. Therefore, I would rate this translation as 5 (excellent).\n",
            "ğŸ” Evaluating:\n",
            "Original: Schedule a meeting with the design team for next Tuesday.\n",
            "Translation: æ¥é€±ã®æ—¥æ›œæ—¥ã«ãƒ‡ã‚¶ã‚¤ãƒ³ãƒãƒ¼ãƒ ã¨ã®ä¼šè­°ã‚’äºˆå®šã—ã¦ãã ã•ã„ã€‚\n",
            "Back-Translation: Please schedule a meeting with the design team for next Sunday.\n",
            "ğŸ§  GPT Response:\n",
            "The back-translation does not preserve the original meaning. The day of the week has been changed from Tuesday to Sunday. This is a significant error as it changes the information about when the meeting is supposed to occur. The tone and accuracy, aside from this error, are preserved. \n",
            "\n",
            "Specific issue: The day of the week was incorrectly translated from Tuesday to Sunday.\n",
            "\n",
            "Rating: 2. The translation is generally good in terms of tone and language use, but the error in the day of the week is a major mistake that could lead to scheduling problems.\n",
            "ğŸ” Evaluating:\n",
            "Original: All passwords must be changed every 90 days.\n",
            "Translation: ã™ã¹ã¦ã®ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ã¯91æ—¥ã”ã¨ã«å¤‰æ›´ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚\n",
            "Back-Translation: All passwords need to be changed every 91 days.\n",
            "ğŸ§  GPT Response:\n",
            "The back-translation does not preserve the original meaning completely. The original text specifies that passwords must be changed every 90 days, but the translation and back-translation state that this must be done every 91 days. This is a minor but significant error in terms of accuracy. The tone and information are otherwise preserved. \n",
            "\n",
            "Specific Issue: The number of days is incorrect in the translation. It should be 90 days, not 91 days.\n",
            "\n",
            "Rating: 4. This translation is generally good, but the error in the number of days lowers the score slightly.\n"
          ]
        }
      ],
      "source": [
        "def evaluate_with_llm_with_back_translation(sample_data):\n",
        "    for item in sample_data:\n",
        "        source = item[\"source_text\"]\n",
        "        translation = item[\"translated_text\"]\n",
        "\n",
        "        # Step 1: Get back-translation\n",
        "        back_translated = get_back_translation(translation)\n",
        "\n",
        "        # Step 2: Build judge prompt with back-translation included\n",
        "        prompt = build_judge_prompt_with_back_translation(source, translation, back_translated)\n",
        "\n",
        "        print(\"ğŸ” Evaluating:\")\n",
        "        print(f\"Original: {source}\")\n",
        "        print(f\"Translation: {translation}\")\n",
        "        print(f\"Back-Translation: {back_translated}\")\n",
        "        print(\"ğŸ§  GPT Response:\")\n",
        "\n",
        "\n",
        "        # Step 3: LLM as judge\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"gpt-4\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are a strict bilingual translation evaluator. Use the back-translation to catch subtle errors.\"},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ],\n",
        "            temperature=0.2\n",
        "        )\n",
        "        print(response.choices[0].message.content)\n",
        "\n",
        "\n",
        "def build_judge_prompt_with_back_translation(source_text, translated_text, back_translated_text):\n",
        "    return f\"\"\"Evaluate the quality of this translation using both the original source and the back-translation.\n",
        "\n",
        "Source (English): {source_text}\n",
        "Translation (Japanese): {translated_text}\n",
        "Back-Translation (English): {back_translated_text}\n",
        "\n",
        "Does the back-translation preserve the original meaning? Are there any changes in tone, information, or accuracy? Please point out specific issues and rate the translation on a scale from 1 (poor) to 5 (excellent).\"\"\"\n",
        "\n",
        "# Example usage:\n",
        "evaluate_with_llm_with_back_translation(sample_data)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ğŸ“Interpretation\n",
        "\n",
        "Back-translation provides an extra layer of validation by checking whether meaning survives when the translation is converted back into English.\n",
        "\n",
        "- **Example 1**:  Matches the original. Minor rephrasing (â€œbeforeâ€ â†’ â€œbyâ€) is stylistic and acceptable.  \n",
        "\n",
        "- **Example 2**: Reveals a critical mismatch: the day of the meeting is wrong. Even though the Japanese sentence is fluent, the round-trip exposes the semantic drift.  \n",
        "\n",
        "- **Example 3**: Shows a numerical inconsistency. While small, such errors can have serious implications in compliance or technical domains.  \n",
        "\n",
        "ğŸ“Œ **Insight:** Back-translation complements direct scoring by uncovering subtle but meaningful errors. It acts as a **round-trip fidelity test** â€” if meaning doesnâ€™t survive the cycle, the translation is unreliable.\n",
        "\n",
        "ğŸš€ **Takeaway:** Combining direct evaluation with back-translation yields a stronger, more trustworthy pipeline. It brings the system closer to **human-level judgment** while remaining automated and scalable.\n"
      ],
      "metadata": {
        "id": "99iOy9XXBSAm"
      },
      "id": "99iOy9XXBSAm"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ğŸ¯ Conclusion\n",
        "\n",
        "In this notebook, we built an **LLM Judge** for evaluating translations.  \n",
        "Through a small dataset, prompt design with few-shot examples, and interactive testing, we explored how an LLM can provide structured feedback instead of relying only on BLEU or human annotation.  \n",
        "\n",
        "### Key Highlights\n",
        "- Evaluations were based on **Accuracy**, **Understandability**, and **Hallucination**.  \n",
        "- The judge worked on both **sample data** and **custom inputs**, showing flexibility.  \n",
        "- An optional **back-translation step** added an extra layer of validation.  \n",
        "\n",
        "### Why It Matters\n",
        "This experiment shows that LLMs can act not only as generators but also as **scalable and reliable evaluators**.  \n",
        "The same approach can be extended to other tasks like summarization, dialogue, or reasoning.  \n"
      ],
      "metadata": {
        "id": "p2L-NaeBBn0P"
      },
      "id": "p2L-NaeBBn0P"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2ZEZ9lVdQqe8"
      },
      "id": "2ZEZ9lVdQqe8",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "vscode",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}